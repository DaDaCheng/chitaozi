{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GCNpan.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNkftnj4A7xXxK9KaNX2gw0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"8vPiRF15LHI9","executionInfo":{"status":"ok","timestamp":1620470208344,"user_tz":-120,"elapsed":8593,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["# Install required packages.\n","!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-geometric\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9M-W0uCmfkzs","executionInfo":{"status":"ok","timestamp":1620470220351,"user_tz":-120,"elapsed":754,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.nn import functional as F\n","from sklearn.metrics import roc_auc_score\n","from torch_geometric.data import Data, DataLoader\n","import torch.nn as nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqLipokJgnvy"},"source":["import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv\n","import torch\n","from torch.nn import Linear\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.utils import to_dense_adj, dense_to_sparse, add_self_loops, degree\n","from torch_geometric.nn import MessagePassing\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","print(torch.cuda.get_device_name(0))\n","\n","def visualize(h, color):\n","    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n","\n","    plt.figure(figsize=(10,10))\n","    plt.xticks([])\n","    plt.yticks([])\n","\n","    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n","    plt.show()\n","\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","dataset = Planetoid(root='data/', name='Cora', transform=NormalizeFeatures())\n","\n","print()\n","print(f'Dataset: {dataset}:')\n","print('======================')\n","print(f'Number of graphs: {len(dataset)}')\n","print(f'Number of features: {dataset.num_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","\n","data = dataset[0]  # Get the first graph object.\n","\n","print()\n","print(data)\n","print('===========================================================================================================')\n","\n","# Gather some statistics about the graph.\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Number of training nodes: {data.train_mask.sum()}')\n","print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n","print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n","print(f'Contains self-loops: {data.contains_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')\n","\n","\n","\n","class FGCNConv(MessagePassing):\n","    def __init__(self, in_channels, out_channels):\n","        super(FGCNConv, self).__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n","        self.lin = torch.nn.Linear(in_channels, out_channels)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        # x has shape [N, in_channels]\n","        # edge_index has shape [2, E]\n","\n","        # Step 1: Add self-loops to the adjacency matrix.\n","        \n","        # Step 2: Linearly transform node feature matrix.\n","        x = self.lin(x)\n","\n","        # Step 3: Compute normalization.\n","        row, col = edge_index\n","        deg_row = degree(row, x.size(0), dtype=x.dtype)\n","        deg_col = degree(col, x.size(0), dtype=x.dtype)\n","        deg_row_inv_sqrt = deg_row.pow(-0.5)\n","        deg_col_inv_sqrt = deg_col.pow(-0.5)\n","        norm = deg_row_inv_sqrt[row] * deg_col_inv_sqrt[col]\n","        norm = norm * edge_attr\n","\n","        # Step 4-5: Start propagating messages.\n","        return self.propagate(edge_index, x=x, norm=norm)\n","\n","    def message(self, x_j, norm):\n","        # x_j has shape [E, out_channels]\n","\n","        # Step 4: Normalize node features.\n","        return norm.view(-1, 1) * x_j\n","\n","beta = 1\n","def PowerInd(edge_index,beta):\n","    # returns the edge list in COO format with shape [2,num_edges,num_2_hop neighbors]\n","    # the edge weighted is stored in the edge_attr, \n","    row, col = edge_index\n","    edge_attr = torch.ones(edge_index.size(1))\n","    edge_attr = torch.mul(edge_attr,-2*beta)\n","    adj = to_dense_adj(edge_index)\n","    adj = torch.matmul(adj,adj)\n","    adj = torch.reshape(adj,(adj.size(1),adj.size(2)))\n","    adj = adj+torch.mul(torch.eye(adj.size(0),dtype=torch.float),beta*beta)\n","    edge_index_f, edge_attr_f = dense_to_sparse(adj)\n","    print(edge_attr_f)\n","    edge_index = torch.cat((edge_index,edge_index_f),dim=1)\n","    edge_attr = torch.cat((edge_attr,edge_attr_f),dim=0)\n","    return edge_index, edge_attr\n","\n","\n","edge_index_f, edge_attr_f = PowerInd(data.edge_index,beta)\n","\n","data_f = Data(edge_index=edge_index_f,x= data.x,edge_attr=edge_attr_f, test_mask=data.test_mask,train_mask=data.train_mask,y=data.y)\n","\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","        self.fconv1 = FGCNConv(dataset.num_features, hidden_channels)       \n","        self.fconv2 = FGCNConv(hidden_channels, dataset.num_classes)\n","        self.w = nn.Parameter(torch.rand(1,1,requires_grad=True))\n","        #self.lin1 = Linear(dataset.num_classes,dataset.num_classes)\n","\n","    def forward(self, x, edge_index, edge_index_f, edge_attr_f):\n","        uf = self.conv1(x, edge_index)\n","        uf = uf.relu()\n","        uf = F.dropout(uf, p=0.5, training=self.training)\n","        uf = self.conv2(uf, edge_index)\n","        \n","\n","        f = self.fconv1(x, edge_index_f, edge_attr_f)\n","        f = f.relu()\n","        f = F.dropout(f, p=0.5, training=self.training)\n","        f = self.fconv2(f, edge_index_f, edge_attr_f)\n","        \n","        uf = f +torch.mul(uf,self.w)\n","        print(self.w)\n","        #uf = torch.cat((uf,f),dim=1)\n","        #uf = uf.relu()\n","        #uf = self.lin1(uf)\n","\n","        return uf\n","\n","#out = model(data.x, data.edge_index)\n","#visualize(out, color=data.y)\n","\n","#from IPython.display import Javascript  # Restrict height of output cell.\n","#display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = GCN(hidden_channels=16).to(device)\n","print(model)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index, data_f.edge_index, data_f.edge_attr)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x, data.edge_index, data_f.edge_index, data_f.edge_attr)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","data = data.to(device)\n","data_f = data_f.to(device)\n","for epoch in range(1, 800):\n","    loss = train()\n","    test_acc = test()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Acc:{test_acc:.4f}')\n","\n","\n","test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLj3VZFn6xlz"},"source":["fname = '/content/drive/My Drive/pan/USAir'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzKlXVnz8kj5","executionInfo":{"status":"ok","timestamp":1620301064198,"user_tz":-120,"elapsed":819,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"26b84eaf-ffbd-44e3-8c59-9683bfd0a508"},"source":["\n","GCN_hidden1 = 400;\n","GCN_hidden2 = 200;\n","GCN_hidden3 = 100;\n","GCN_hidden4 = 50;\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","print(torch.cuda.get_device_name(0))\n","\n","def non_ptb_data(index, n, x, y):\n","    # prepare the non-perturbed data\n","    edge_index = np.concatenate((index,index[:,[1,0]]),axis=0)\n","    edge_index = torch.tensor(edge_index,dtype=torch.long)\n","    x = torch.as_tensor(x,dtype=torch.float)\n","    y = torch.as_tensor(y,dtype=torch.float)\n","    data = Data(x=x,edge_index=edge_index.t().contiguous(),num_nodes=n, y=y)\n","    return data\n","\n","def ptb_data_train(index, n, x, label):\n","    # prepare the perturbed data with the edge (s,t) added or removed\n","    # if label == 1 then remove the edge, and add the edge otherwise\n","    labels = np.zeros((1,2))\n","    labels[0,0] = label[2]\n","    labels[0,1] = label[2]\n","    if label[2] == 0:\n","        edge = np.zeros((1,2))\n","        edge[0,0] = label[0]\n","        edge[0,1] = label[1]\n","        edge_index = np.concatenate((index,edge),axis=0)\n","        data = non_ptb_data(edge_index,n,x,labels)\n","    else:\n","        ind = np.where((index == (label[0], label[1])).all(axis=1))\n","        edge_index = np.delete(index,ind[0],0)\n","        data = non_ptb_data(edge_index,n,x,labels)\n","    return data\n","\n","def ptb_data_test(index, n, x, label):\n","    # prepare the perturbed data with the edge (s,t) added or removed\n","    # if label == 1 then remove the edge, and add the edge otherwise\n","    labels = np.zeros((1,2))\n","    labels[0,0] = 0\n","    labels[0,1] = label[2]\n","\n","    edge = np.zeros((1,2))\n","    edge[0,0] = label[0]\n","    edge[0,1] = label[1]\n","    edge_index = np.concatenate((index,edge),axis=0)\n","    data = non_ptb_data(edge_index,n,x,labels)\n","    return data\n","\n","\n","\n","\n","\n","class MLP(torch.nn.Module):\n","    def __init__(self,input_size):\n","        super(MLP, self).__init__()\n","        self.linear1 = torch.nn.Linear(input_size,200)\n","        self.linear2 = torch.nn.Linear(200,100)\n","        self.linear3 = torch.nn.Linear(100,80)\n","        self.linear4 = torch.nn.Linear(80,20)\n","        self.linear5 = torch.nn.Linear(20,1)\n","        self.act1= nn.ReLU()\n","        #self.act2= nn.ReLU()\n","        #self.act3= nn.ReLU()\n","        self.act2= torch.sin\n","        self.act3= torch.sin\n","        self.act4= nn.ReLU()\n","\n","    def forward(self, x):\n","        out= self.linear1(x)\n","        out = self.act1(out)\n","        out= self.linear2(out)\n","        out = self.act2(out)\n","        out = self.linear3(out)\n","        out = self.act3(out)\n","        out = self.linear4(out)\n","        out = self.act4(out)\n","        out = self.linear5(out)\n","        out = torch.sigmoid(out)\n","        return out\n","\n","class Net(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(Net, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(non_perturb_train.num_node_features, GCN_hidden1)\n","        self.conv2 = GCNConv(GCN_hidden1, GCN_hidden2)\n","        self.conv3 = GCNConv(GCN_hidden2, GCN_hidden3)\n","        self.conv4 = GCNConv(GCN_hidden3, GCN_hidden4)\n","        self.mlp = MLP((GCN_hidden1+GCN_hidden2+GCN_hidden3+GCN_hidden4)*n)\n","\n","    def forward(self, x_p, x_np, y, edge_index_p,edge_index_np):\n","        o_p_1 = self.conv1(x_p, edge_index_p)\n","        o_p_1 = o_p_1.relu()\n","        #o_p_1 = torch.sin(o_p_1)\n","        #o_p_1 = F.dropout(o_p_1, p=0.1, training=self.training)\n","    \n","        o_p_2 = self.conv2(o_p_1, edge_index_p)\n","        o_p_2 = o_p_2.relu()\n","        #o_p_2 = torch.sin(o_p_2)\n","        #o_p_2 = F.dropout(o_p_2, p=0.5, training=self.training)\n","\n","        o_p_3 = self.conv3(o_p_2, edge_index_p)\n","        o_p_3 = o_p_3.relu()\n","\n","        o_p_4 = self.conv4(o_p_3, edge_index_p)\n","       \n","        o_np_1 = self.conv1(x_np, edge_index_np)\n","        #o_np_1 = torch.sin(o_np_1)\n","        o_np_1 = o_np_1.relu()\n","        #o_np_1 = F.dropout(o_np_1, p=0.5, training=self.training)\n","    \n","        o_np_2 = self.conv2(o_np_1, edge_index_np)\n","        o_np_2 = o_np_2.relu()\n","        #o_np_2 = torch.sin(o_np_2)\n","        #o_np_2 = F.dropout(o_np_2, p=0.5, training=self.training)\n","\n","        o_np_3 = self.conv3(o_np_2, edge_index_np)\n","        o_np_3 = o_np_3.relu()\n","\n","        o_np_4 = self.conv4(o_np_3, edge_index_np)\n","\n","\n","        o_p = torch.cat((o_p_1,o_p_2,o_p_3,o_p_4),dim= 1)\n","        o_np = torch.cat((o_np_1,o_np_2,o_np_3,o_np_4),dim= 1)\n","        \n","        factor = y.size(0)\n","        z = o_np.repeat(factor,1)\n","        z = z- o_p\n","        out = torch.reshape(torch.mul(z[0:n,:],(y[0,0]-0.5)*2),(1,n*z.size(1)))\n","        for i in range(1,factor):\n","            tmp = torch.reshape(torch.mul(z[i*n:i*n+n,:],(y[i,0]-0.5)*2),(1,n*z.size(1)))\n","            out = torch.cat((out,tmp),dim = 0)\n","        z = self.mlp(out)\n","\n","        return z"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"toBiFMErsHb1"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"j_VL_lEj9GF0","executionInfo":{"status":"error","timestamp":1620151626655,"user_tz":-120,"elapsed":983,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"c5be2d12-6318-42f2-851e-a96f8d0a2414"},"source":["for step, data in enumerate(train_loader):\n","    print(f'Step {step + 1}:')\n","    print('=======')\n","    print(f'Number of graphs in the current batch: {data.num_graphs}')\n","    print(data)\n","    print()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a2e520a46ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Step {step + 1}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Number of graphs in the current batch: {data.num_graphs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"VzbooGqvIQkB"},"source":["n = 332\n","batch_size = 64\n","x = torch.eye(n,dtype=torch.float)\n","#x = torch.ones((n,1))\n","train_data = pd.read_csv(fname+'_train_el.csv',header=None)\n","train_edge_index = np.array(train_data.iloc[:,0:])\n","non_perturb_train = non_ptb_data(train_edge_index,n,x,np.zeros((1,2))).to(device)\n","\n","train_labels = pd.read_csv(fname+'_train_labels.csv',header=None)\n","train_labels = np.array(train_labels.iloc[:,0:])\n","test_labels = pd.read_csv(fname+'_test_labels.csv',header=None)\n","test_labels = np.array(test_labels.iloc[:,0:])\n","\n","train_data_list = [ptb_data_train(train_edge_index,n,x,train_labels[i,0:3]) for i in range(0,len(train_labels))]\n","train_loader = DataLoader(train_data_list,batch_size=batch_size,shuffle=True)\n","test_data_list = [ptb_data_test(train_edge_index,n,x,test_labels[i,0:3]) for i in range(0,len(test_labels))]\n","test_loader = DataLoader(test_data_list,batch_size=int(len(test_data_list)),shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KDbLqPZev-k"},"source":["# def labelx(x,non_adj,edge_index):\n","#     with torch.no_grad():\n","#         x_new=x.clone().detach()\n","        \n","#         per_adj=torch.zeros_like(non_adj)\n","#         per_small_adj=to_dense_adj(edge_index)\n","#         a,b=per_small_adj[0].shape\n","#         per_adj[0][:a,:b]=per_small_adj[0]\n","#         diffadj=non_adj-per_adj\n","#         diffnode=torch.where(diffadj[0]!=0)\n","#         for nodes in diffnode:\n","#             x_new[nodes[0]][nodes[1]]=-diffadj[0][nodes[0]][nodes[1]]\n","#             #x_new[nodes[0]][nodes[0]]=1\n","#             #x_new[nodes[1]][nodes[1]]=1\n","#     return x_new\n","\n","        \n","\n","\n","# n = 332\n","# batch_size = 32\n","# x = torch.eye(n,dtype=torch.float)\n","# #x = torch.ones((n,1))\n","# train_data = pd.read_csv(fname+'_train_el.csv',header=None)\n","# train_edge_index = np.array(train_data.iloc[:,0:])\n","# non_perturb_train = non_ptb_data(train_edge_index,n,x,np.zeros((1,2))).to(device)\n","\n","# train_labels = pd.read_csv(fname+'_train_labels.csv',header=None)\n","# train_labels = np.array(train_labels.iloc[:,0:])\n","# test_labels = pd.read_csv(fname+'_test_labels.csv',header=None)\n","# test_labels = np.array(test_labels.iloc[:,0:])\n","\n","\n","# non_adj=to_dense_adj(non_perturb_train.edge_index).cpu()\n","# train_data_list = [ptb_data_train(train_edge_index,n,x,train_labels[i,0:3]) for i in range(0,len(train_labels))]\n","# train_data_list_label=[]\n","# x = torch.eye(n,dtype=torch.float)\n","# for data in train_data_list:\n","#     data.x=labelx(x,non_adj,data.edge_index)\n","#     train_data_list_label.append(data)\n","# train_loader = DataLoader(train_data_list_label,batch_size=batch_size,shuffle=True)\n","\n","\n","\n","# test_data_list = [ptb_data_test(train_edge_index,n,x,test_labels[i,0:3]) for i in range(0,len(test_labels))]\n","# test_data_list_label=[]\n","# x = torch.eye(n,dtype=torch.float)\n","# for data in test_data_list:\n","#     data.x=labelx(x,non_adj,data.edge_index)\n","#     test_data_list_label.append(data)\n","# test_loader = DataLoader(test_data_list_label,batch_size=len(test_data_list),shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_bGqfOV0bgu","executionInfo":{"status":"ok","timestamp":1620155990851,"user_tz":-120,"elapsed":693,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"50af5d01-3f6b-4c56-8a12-50dd35e28fb1"},"source":["data.x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 1., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 1.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 1., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 1., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":154}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HQ2dNqaVOuF","executionInfo":{"status":"ok","timestamp":1620158729165,"user_tz":-120,"elapsed":102158,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"b6c7d772-cca5-449d-a758-b7761dd4dd6d"},"source":["model = Net(hidden_channels=12).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","#criterion = torch.nn.BCEWithLogitsLoss()\n","criterion = torch.nn.BCELoss()\n","def train(train_loader):\n","    model.train()\n","    loss_eposch=0\n","    for step, data in enumerate(train_loader):# Iterate in batches over the training dataset.\n","        data = data.to(device)\n","        out = model(data.x, non_perturb_train.x, data.y, data.edge_index, non_perturb_train.edge_index) \n","        out = out.reshape(-1).to(device) # Perform a single forward pass.\n","        #print('aa',out)\n","        #print('bb',data.y[:,0])\n","        loss = criterion(out, data.y[:,0])  # Compute the loss.\n","        loss.backward()  # Derive gradients.\n","        optimizer.step()  # Update parameters based on gradients.\n","        optimizer.zero_grad()  # Clear gradients.\n","        #print(f\"Step:{step},Loss:{loss.item()}\")\n","        loss_eposch=loss_eposch+loss.item()\n","    return loss_eposch/(step+1.0)\n","\n","def test(loader):\n","     model.eval()\n","\n","     correct = 0\n","     for step, data in enumerate(loader):  # Iterate in batches over the training/test dataset.\n","         #print(f'Num of graphs in the current batch: {data.num_graphs}')\n","         data = data.to(device)\n","         out = model(data.x, non_perturb_train.x, data.y, data.edge_index, non_perturb_train.edge_index) \n","         scores = out.cpu().detach().numpy()\n","         #print(scores)\n","         labels = data.y[:,1].cpu().detach().numpy()\n","         break\n","     return roc_auc_score(labels, scores)  # Derive ratio of correct predictions.\n","\n","num_epochs = 20\n","for epoch in range(0, num_epochs):\n","    loss_step = train(train_loader)\n","    train_acc = test(train_loader)\n","    test_acc = test(test_loader)\n","    print('Epoch [{}/{}], Loss: {:.4f}, Train AUC: {:.4f}, Test AUC: {:.4f}'\\\n","                .format(epoch+1, num_epochs, loss_step, train_acc, test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/20], Loss: 0.5286, Train AUC: 0.9650, Test AUC: 0.9309\n","Epoch [2/20], Loss: 0.1636, Train AUC: 1.0000, Test AUC: 0.9543\n","Epoch [3/20], Loss: 0.0570, Train AUC: 1.0000, Test AUC: 0.9543\n","Epoch [4/20], Loss: 0.0269, Train AUC: 1.0000, Test AUC: 0.9566\n","Epoch [5/20], Loss: 0.0155, Train AUC: 1.0000, Test AUC: 0.9534\n","Epoch [6/20], Loss: 0.0053, Train AUC: 1.0000, Test AUC: 0.9518\n","Epoch [7/20], Loss: 0.0070, Train AUC: 1.0000, Test AUC: 0.9568\n","Epoch [8/20], Loss: 0.0029, Train AUC: 1.0000, Test AUC: 0.9546\n","Epoch [9/20], Loss: 0.0007, Train AUC: 1.0000, Test AUC: 0.9561\n","Epoch [10/20], Loss: 0.0004, Train AUC: 1.0000, Test AUC: 0.9569\n","Epoch [11/20], Loss: 0.0002, Train AUC: 1.0000, Test AUC: 0.9576\n","Epoch [12/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9577\n","Epoch [13/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9579\n","Epoch [14/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9579\n","Epoch [15/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9581\n","Epoch [16/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9581\n","Epoch [17/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9582\n","Epoch [18/20], Loss: 0.0000, Train AUC: 1.0000, Test AUC: 0.9583\n","Epoch [19/20], Loss: 0.0000, Train AUC: 1.0000, Test AUC: 0.9584\n","Epoch [20/20], Loss: 0.0000, Train AUC: 1.0000, Test AUC: 0.9585\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IRJGmVcUewlP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNyCwM8SIQd6"},"source":[""]}]}