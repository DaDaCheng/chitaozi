{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GCNpan.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyO+O8XHXpXDAT4qZcYv6mHw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByXEnmyQ8i3A","executionInfo":{"status":"ok","timestamp":1620154367144,"user_tz":-120,"elapsed":1257,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"4a5f09b2-1cf2-401b-ce7d-c29c2b6462f8"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as Data\n","from torch.nn import functional as F\n","from sklearn.metrics import roc_auc_score \n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","import sys\n","DIR='/content/drive/My\\ Drive/pan/'\n","sys.path.append(DIR)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' \n","print('Using {} device'.format(device))\n","from torch_geometric.utils import to_dense_adj"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Using cuda device\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8vPiRF15LHI9","executionInfo":{"status":"ok","timestamp":1620154376051,"user_tz":-120,"elapsed":8074,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["# Install required packages.\n","!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-geometric\n"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"9M-W0uCmfkzs","executionInfo":{"status":"ok","timestamp":1620154376052,"user_tz":-120,"elapsed":3016,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.nn import functional as F\n","from sklearn.metrics import roc_auc_score\n","from torch_geometric.data import Data, DataLoader\n","import torch.nn as nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLj3VZFn6xlz","executionInfo":{"status":"ok","timestamp":1620154376052,"user_tz":-120,"elapsed":1273,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["fname = '/content/drive/My Drive/pan/USAir'"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzKlXVnz8kj5","executionInfo":{"status":"ok","timestamp":1620158624109,"user_tz":-120,"elapsed":675,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"7784dc5c-a693-448e-ae87-a413d2d26d2e"},"source":["\n","GCN_hidden1 = 400;\n","GCN_hidden2 = 200;\n","GCN_hidden3 = 100;\n","GCN_hidden4 = 50;\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","print(torch.cuda.get_device_name(0))\n","\n","def non_ptb_data(index, n, x, y):\n","    # prepare the non-perturbed data\n","    edge_index = np.concatenate((index,index[:,[1,0]]),axis=0)\n","    edge_index = torch.tensor(edge_index,dtype=torch.long)\n","    x = torch.as_tensor(x,dtype=torch.float)\n","    y = torch.as_tensor(y,dtype=torch.float)\n","    data = Data(x=x,edge_index=edge_index.t().contiguous(),num_nodes=n, y=y)\n","    return data\n","\n","def ptb_data_train(index, n, x, label):\n","    # prepare the perturbed data with the edge (s,t) added or removed\n","    # if label == 1 then remove the edge, and add the edge otherwise\n","    labels = np.zeros((1,2))\n","    labels[0,0] = label[2]\n","    labels[0,1] = label[2]\n","    if label[2] == 0:\n","        edge = np.zeros((1,2))\n","        edge[0,0] = label[0]\n","        edge[0,1] = label[1]\n","        edge_index = np.concatenate((index,edge),axis=0)\n","        data = non_ptb_data(edge_index,n,x,labels)\n","    else:\n","        ind = np.where((index == (label[0], label[1])).all(axis=1))\n","        edge_index = np.delete(index,ind[0],0)\n","        data = non_ptb_data(edge_index,n,x,labels)\n","    return data\n","\n","def ptb_data_test(index, n, x, label):\n","    # prepare the perturbed data with the edge (s,t) added or removed\n","    # if label == 1 then remove the edge, and add the edge otherwise\n","    labels = np.zeros((1,2))\n","    labels[0,0] = 0\n","    labels[0,1] = label[2]\n","\n","    edge = np.zeros((1,2))\n","    edge[0,0] = label[0]\n","    edge[0,1] = label[1]\n","    edge_index = np.concatenate((index,edge),axis=0)\n","    data = non_ptb_data(edge_index,n,x,labels)\n","    return data\n","\n","\n","\n","\n","\n","class MLP(torch.nn.Module):\n","    def __init__(self,input_size):\n","        super(MLP, self).__init__()\n","        self.linear1 = torch.nn.Linear(input_size,200)\n","        self.linear2 = torch.nn.Linear(200,100)\n","        self.linear3 = torch.nn.Linear(100,80)\n","        self.linear4 = torch.nn.Linear(80,20)\n","        self.linear5 = torch.nn.Linear(20,1)\n","        self.act1= nn.ReLU()\n","        #self.act2= nn.ReLU()\n","        #self.act3= nn.ReLU()\n","        self.act2= torch.sin\n","        self.act3= torch.sin\n","        self.act4= nn.ReLU()\n","\n","    def forward(self, x):\n","        out= self.linear1(x)\n","        out = self.act1(out)\n","        out= self.linear2(out)\n","        out = self.act2(out)\n","        out = self.linear3(out)\n","        out = self.act3(out)\n","        out = self.linear4(out)\n","        out = self.act4(out)\n","        out = self.linear5(out)\n","        out = torch.sigmoid(out)\n","        return out\n","\n","class Net(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(Net, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(non_perturb_train.num_node_features, GCN_hidden1)\n","        self.conv2 = GCNConv(GCN_hidden1, GCN_hidden2)\n","        self.conv3 = GCNConv(GCN_hidden2, GCN_hidden3)\n","        self.conv4 = GCNConv(GCN_hidden3, GCN_hidden4)\n","        self.mlp = MLP((GCN_hidden1+GCN_hidden2+GCN_hidden3+GCN_hidden4)*n)\n","\n","    def forward(self, x_p, x_np, y, edge_index_p,edge_index_np):\n","        o_p_1 = self.conv1(x_p, edge_index_p)\n","        o_p_1 = o_p_1.relu()\n","        #o_p_1 = torch.sin(o_p_1)\n","        #o_p_1 = F.dropout(o_p_1, p=0.1, training=self.training)\n","    \n","        o_p_2 = self.conv2(o_p_1, edge_index_p)\n","        o_p_2 = o_p_2.relu()\n","        #o_p_2 = torch.sin(o_p_2)\n","        #o_p_2 = F.dropout(o_p_2, p=0.5, training=self.training)\n","\n","        o_p_3 = self.conv3(o_p_2, edge_index_p)\n","        o_p_3 = o_p_3.relu()\n","\n","        o_p_4 = self.conv4(o_p_3, edge_index_p)\n","       \n","        o_np_1 = self.conv1(x_np, edge_index_np)\n","        #o_np_1 = torch.sin(o_np_1)\n","        o_np_1 = o_np_1.relu()\n","        #o_np_1 = F.dropout(o_np_1, p=0.5, training=self.training)\n","    \n","        o_np_2 = self.conv2(o_np_1, edge_index_np)\n","        o_np_2 = o_np_2.relu()\n","        #o_np_2 = torch.sin(o_np_2)\n","        #o_np_2 = F.dropout(o_np_2, p=0.5, training=self.training)\n","\n","        o_np_3 = self.conv3(o_np_2, edge_index_np)\n","        o_np_3 = o_np_3.relu()\n","\n","        o_np_4 = self.conv4(o_np_3, edge_index_np)\n","\n","\n","        o_p = torch.cat((o_p_1,o_p_2,o_p_3,o_p_4),dim= 1)\n","        o_np = torch.cat((o_np_1,o_np_2,o_np_3,o_np_4),dim= 1)\n","        \n","        factor = y.size(0)\n","        z = o_np.repeat(factor,1)\n","        z = z- o_p\n","        out = torch.reshape(torch.mul(z[0:n,:],(y[0,0]-0.5)*2),(1,n*z.size(1)))\n","        for i in range(1,factor):\n","            tmp = torch.reshape(torch.mul(z[i*n:i*n+n,:],(y[i,0]-0.5)*2),(1,n*z.size(1)))\n","            out = torch.cat((out,tmp),dim = 0)\n","        z = self.mlp(out)\n","\n","        return z"],"execution_count":215,"outputs":[{"output_type":"stream","text":["cuda\n","Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"toBiFMErsHb1","executionInfo":{"status":"ok","timestamp":1620156151883,"user_tz":-120,"elapsed":609,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["\n"],"execution_count":163,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"j_VL_lEj9GF0","executionInfo":{"status":"error","timestamp":1620151626655,"user_tz":-120,"elapsed":983,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"c5be2d12-6318-42f2-851e-a96f8d0a2414"},"source":["for step, data in enumerate(train_loader):\n","    print(f'Step {step + 1}:')\n","    print('=======')\n","    print(f'Number of graphs in the current batch: {data.num_graphs}')\n","    print(data)\n","    print()"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a2e520a46ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Step {step + 1}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Number of graphs in the current batch: {data.num_graphs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"VzbooGqvIQkB","executionInfo":{"status":"ok","timestamp":1620158222744,"user_tz":-120,"elapsed":1116,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["n = 332\n","batch_size = 64\n","x = torch.eye(n,dtype=torch.float)\n","#x = torch.ones((n,1))\n","train_data = pd.read_csv(fname+'_train_el.csv',header=None)\n","train_edge_index = np.array(train_data.iloc[:,0:])\n","non_perturb_train = non_ptb_data(train_edge_index,n,x,np.zeros((1,2))).to(device)\n","\n","train_labels = pd.read_csv(fname+'_train_labels.csv',header=None)\n","train_labels = np.array(train_labels.iloc[:,0:])\n","test_labels = pd.read_csv(fname+'_test_labels.csv',header=None)\n","test_labels = np.array(test_labels.iloc[:,0:])\n","\n","train_data_list = [ptb_data_train(train_edge_index,n,x,train_labels[i,0:3]) for i in range(0,len(train_labels))]\n","train_loader = DataLoader(train_data_list,batch_size=batch_size,shuffle=True)\n","test_data_list = [ptb_data_test(train_edge_index,n,x,test_labels[i,0:3]) for i in range(0,len(test_labels))]\n","test_loader = DataLoader(test_data_list,batch_size=int(len(test_data_list)),shuffle=True)"],"execution_count":209,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KDbLqPZev-k","executionInfo":{"status":"ok","timestamp":1620157626341,"user_tz":-120,"elapsed":889,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["# def labelx(x,non_adj,edge_index):\n","#     with torch.no_grad():\n","#         x_new=x.clone().detach()\n","        \n","#         per_adj=torch.zeros_like(non_adj)\n","#         per_small_adj=to_dense_adj(edge_index)\n","#         a,b=per_small_adj[0].shape\n","#         per_adj[0][:a,:b]=per_small_adj[0]\n","#         diffadj=non_adj-per_adj\n","#         diffnode=torch.where(diffadj[0]!=0)\n","#         for nodes in diffnode:\n","#             x_new[nodes[0]][nodes[1]]=-diffadj[0][nodes[0]][nodes[1]]\n","#             #x_new[nodes[0]][nodes[0]]=1\n","#             #x_new[nodes[1]][nodes[1]]=1\n","#     return x_new\n","\n","        \n","\n","\n","# n = 332\n","# batch_size = 32\n","# x = torch.eye(n,dtype=torch.float)\n","# #x = torch.ones((n,1))\n","# train_data = pd.read_csv(fname+'_train_el.csv',header=None)\n","# train_edge_index = np.array(train_data.iloc[:,0:])\n","# non_perturb_train = non_ptb_data(train_edge_index,n,x,np.zeros((1,2))).to(device)\n","\n","# train_labels = pd.read_csv(fname+'_train_labels.csv',header=None)\n","# train_labels = np.array(train_labels.iloc[:,0:])\n","# test_labels = pd.read_csv(fname+'_test_labels.csv',header=None)\n","# test_labels = np.array(test_labels.iloc[:,0:])\n","\n","\n","# non_adj=to_dense_adj(non_perturb_train.edge_index).cpu()\n","# train_data_list = [ptb_data_train(train_edge_index,n,x,train_labels[i,0:3]) for i in range(0,len(train_labels))]\n","# train_data_list_label=[]\n","# x = torch.eye(n,dtype=torch.float)\n","# for data in train_data_list:\n","#     data.x=labelx(x,non_adj,data.edge_index)\n","#     train_data_list_label.append(data)\n","# train_loader = DataLoader(train_data_list_label,batch_size=batch_size,shuffle=True)\n","\n","\n","\n","# test_data_list = [ptb_data_test(train_edge_index,n,x,test_labels[i,0:3]) for i in range(0,len(test_labels))]\n","# test_data_list_label=[]\n","# x = torch.eye(n,dtype=torch.float)\n","# for data in test_data_list:\n","#     data.x=labelx(x,non_adj,data.edge_index)\n","#     test_data_list_label.append(data)\n","# test_loader = DataLoader(test_data_list_label,batch_size=len(test_data_list),shuffle=False)"],"execution_count":193,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_bGqfOV0bgu","executionInfo":{"status":"ok","timestamp":1620155990851,"user_tz":-120,"elapsed":693,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"50af5d01-3f6b-4c56-8a12-50dd35e28fb1"},"source":["data.x"],"execution_count":154,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 1., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 1.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 1., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 1., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":154}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HQ2dNqaVOuF","executionInfo":{"status":"ok","timestamp":1620158729165,"user_tz":-120,"elapsed":102158,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"b6c7d772-cca5-449d-a758-b7761dd4dd6d"},"source":["model = Net(hidden_channels=12).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","#criterion = torch.nn.BCEWithLogitsLoss()\n","criterion = torch.nn.BCELoss()\n","def train(train_loader):\n","    model.train()\n","    loss_eposch=0\n","    for step, data in enumerate(train_loader):# Iterate in batches over the training dataset.\n","        data = data.to(device)\n","        out = model(data.x, non_perturb_train.x, data.y, data.edge_index, non_perturb_train.edge_index) \n","        out = out.reshape(-1).to(device) # Perform a single forward pass.\n","        #print('aa',out)\n","        #print('bb',data.y[:,0])\n","        loss = criterion(out, data.y[:,0])  # Compute the loss.\n","        loss.backward()  # Derive gradients.\n","        optimizer.step()  # Update parameters based on gradients.\n","        optimizer.zero_grad()  # Clear gradients.\n","        #print(f\"Step:{step},Loss:{loss.item()}\")\n","        loss_eposch=loss_eposch+loss.item()\n","    return loss_eposch/(step+1.0)\n","\n","def test(loader):\n","     model.eval()\n","\n","     correct = 0\n","     for step, data in enumerate(loader):  # Iterate in batches over the training/test dataset.\n","         #print(f'Num of graphs in the current batch: {data.num_graphs}')\n","         data = data.to(device)\n","         out = model(data.x, non_perturb_train.x, data.y, data.edge_index, non_perturb_train.edge_index) \n","         scores = out.cpu().detach().numpy()\n","         #print(scores)\n","         labels = data.y[:,1].cpu().detach().numpy()\n","         break\n","     return roc_auc_score(labels, scores)  # Derive ratio of correct predictions.\n","\n","num_epochs = 20\n","for epoch in range(0, num_epochs):\n","    loss_step = train(train_loader)\n","    train_acc = test(train_loader)\n","    test_acc = test(test_loader)\n","    print('Epoch [{}/{}], Loss: {:.4f}, Train AUC: {:.4f}, Test AUC: {:.4f}'\\\n","                .format(epoch+1, num_epochs, loss_step, train_acc, test_acc))"],"execution_count":216,"outputs":[{"output_type":"stream","text":["Epoch [1/20], Loss: 0.5286, Train AUC: 0.9650, Test AUC: 0.9309\n","Epoch [2/20], Loss: 0.1636, Train AUC: 1.0000, Test AUC: 0.9543\n","Epoch [3/20], Loss: 0.0570, Train AUC: 1.0000, Test AUC: 0.9543\n","Epoch [4/20], Loss: 0.0269, Train AUC: 1.0000, Test AUC: 0.9566\n","Epoch [5/20], Loss: 0.0155, Train AUC: 1.0000, Test AUC: 0.9534\n","Epoch [6/20], Loss: 0.0053, Train AUC: 1.0000, Test AUC: 0.9518\n","Epoch [7/20], Loss: 0.0070, Train AUC: 1.0000, Test AUC: 0.9568\n","Epoch [8/20], Loss: 0.0029, Train AUC: 1.0000, Test AUC: 0.9546\n","Epoch [9/20], Loss: 0.0007, Train AUC: 1.0000, Test AUC: 0.9561\n","Epoch [10/20], Loss: 0.0004, Train AUC: 1.0000, Test AUC: 0.9569\n","Epoch [11/20], Loss: 0.0002, Train AUC: 1.0000, Test AUC: 0.9576\n","Epoch [12/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9577\n","Epoch [13/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9579\n","Epoch [14/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9579\n","Epoch [15/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9581\n","Epoch [16/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9581\n","Epoch [17/20], Loss: 0.0001, Train AUC: 1.0000, Test AUC: 0.9582\n","Epoch [18/20], Loss: 0.0000, Train AUC: 1.0000, Test AUC: 0.9583\n","Epoch [19/20], Loss: 0.0000, Train AUC: 1.0000, Test AUC: 0.9584\n","Epoch [20/20], Loss: 0.0000, Train AUC: 1.0000, Test AUC: 0.9585\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IRJGmVcUewlP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNyCwM8SIQd6"},"source":[""]}]}